{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from numpy import save, load\n",
    "from keras.engine.topology import Layer\n",
    "from pandas.io.json import json_normalize\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import activations, initializers, constraints, regularizers\n",
    "from tensorflow.keras.layers import Permute, concatenate, Attention, MultiHeadAttention\n",
    "pp = pprint.PrettyPrinter(indent=4, width=80, compact=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../data/HealthStory_tweets_processed.csv\", lineterminator='\\n', index_col=0)\n",
    "health_story = pd.read_csv(\"../data/HealthStory.csv\")\n",
    "news_articles = sorted(health_story['story_review_no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet.created_at</th>\n",
       "      <th>tweet.entities.hashtags</th>\n",
       "      <th>tweet.entities.symbols</th>\n",
       "      <th>tweet.entities.user_mentions</th>\n",
       "      <th>tweet.id</th>\n",
       "      <th>tweet.lang</th>\n",
       "      <th>tweet.place</th>\n",
       "      <th>tweet.retweet_count</th>\n",
       "      <th>tweet.source</th>\n",
       "      <th>tweet.favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet.user.location</th>\n",
       "      <th>tweet.user.default_profile</th>\n",
       "      <th>tweet.user.default_profile_image</th>\n",
       "      <th>tweet.user_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>tweet.possibly_sensitive</th>\n",
       "      <th>username_words</th>\n",
       "      <th>description_words</th>\n",
       "      <th>user_creation_to_tweet_time_days</th>\n",
       "      <th>user_tweeting_freq_per_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-27 19:46:29+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8291515579</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;a href=\"http://twitterfeed.com\" rel=\"nofollow...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>41784143</td>\n",
       "      <td>story_reviews_01660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053178</td>\n",
       "      <td>1.823518e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-27 23:32:01+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8299477970</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>84270990</td>\n",
       "      <td>story_reviews_01660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020736</td>\n",
       "      <td>2.825444e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-28 22:51:27+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8341905171</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>86938456</td>\n",
       "      <td>story_reviews_01660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>3.510424e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2010-02-03 21:07:54+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'text': 'ICAD', 'indices': [0, 5]}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8603224332</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>35253833</td>\n",
       "      <td>story_reviews_01660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060346</td>\n",
       "      <td>2.638592e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-28 03:48:59+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8308211999</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;a href=\"http://twitterfeed.com\" rel=\"nofollow...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>27561937</td>\n",
       "      <td>story_reviews_01660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064569</td>\n",
       "      <td>3.024342e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet.created_at tweet.entities.hashtags  \\\n",
       "0  2010-01-27 19:46:29+00:00                      []   \n",
       "1  2010-01-27 23:32:01+00:00                      []   \n",
       "2  2010-01-28 22:51:27+00:00                      []   \n",
       "3  2010-02-03 21:07:54+00:00                      []   \n",
       "4  2010-01-28 03:48:59+00:00                      []   \n",
       "\n",
       "                  tweet.entities.symbols tweet.entities.user_mentions  \\\n",
       "0                                     []                           []   \n",
       "1                                     []                           []   \n",
       "2                                     []                           []   \n",
       "3  [{'text': 'ICAD', 'indices': [0, 5]}]                           []   \n",
       "4                                     []                           []   \n",
       "\n",
       "     tweet.id tweet.lang  tweet.place  tweet.retweet_count  \\\n",
       "0  8291515579         en          NaN                  0.0   \n",
       "1  8299477970         en          NaN                  0.0   \n",
       "2  8341905171         en          NaN                  0.0   \n",
       "3  8603224332         en          NaN                  0.0   \n",
       "4  8308211999         en          NaN                  0.0   \n",
       "\n",
       "                                        tweet.source  tweet.favorite_count  \\\n",
       "0  <a href=\"http://twitterfeed.com\" rel=\"nofollow...                   0.0   \n",
       "1  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...                   0.0   \n",
       "2  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...                   0.0   \n",
       "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...                   0.0   \n",
       "4  <a href=\"http://twitterfeed.com\" rel=\"nofollow...                   0.0   \n",
       "\n",
       "   ...  tweet.user.location tweet.user.default_profile  \\\n",
       "0  ...                    1                          1   \n",
       "1  ...                    1                          0   \n",
       "2  ...                    0                          0   \n",
       "3  ...                    1                          1   \n",
       "4  ...                    0                          0   \n",
       "\n",
       "  tweet.user.default_profile_image  tweet.user_id              news_id  \\\n",
       "0                             True       41784143  story_reviews_01660   \n",
       "1                            False       84270990  story_reviews_01660   \n",
       "2                            False       86938456  story_reviews_01660   \n",
       "3                            False       35253833  story_reviews_01660   \n",
       "4                            False       27561937  story_reviews_01660   \n",
       "\n",
       "   tweet.possibly_sensitive  username_words description_words  \\\n",
       "0                       NaN        0.000000          0.000000   \n",
       "1                       NaN        0.000000          0.000000   \n",
       "2                       NaN        0.142857          0.051471   \n",
       "3                       NaN        0.071429          0.000000   \n",
       "4                       NaN        0.000000          0.000000   \n",
       "\n",
       "   user_creation_to_tweet_time_days  user_tweeting_freq_per_day  \n",
       "0                          0.053178                1.823518e-05  \n",
       "1                          0.020736                2.825444e-05  \n",
       "2                          0.018561                3.510424e-07  \n",
       "3                          0.060346                2.638592e-04  \n",
       "4                          0.064569                3.024342e-05  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet.created_at', 'tweet.entities.hashtags', 'tweet.entities.symbols',\n",
       "       'tweet.entities.user_mentions', 'tweet.id', 'tweet.lang', 'tweet.place',\n",
       "       'tweet.retweet_count', 'tweet.source', 'tweet.favorite_count',\n",
       "       'tweet.retweeted', 'tweet.text', 'tweet.user.description',\n",
       "       'tweet.user.protected', 'tweet.user.followers_count',\n",
       "       'tweet.user.friends_count', 'tweet.user.listed_count',\n",
       "       'tweet.user.created_at', 'tweet.user.favourites_count',\n",
       "       'tweet.user.verified', 'tweet.user.following',\n",
       "       'tweet.user.statuses_count', 'tweet.user.lang', 'tweet.user.name',\n",
       "       'tweet.user.geo_enabled', 'tweet.user.location',\n",
       "       'tweet.user.default_profile', 'tweet.user.default_profile_image',\n",
       "       'tweet.user_id', 'news_id', 'tweet.possibly_sensitive',\n",
       "       'username_words', 'description_words',\n",
       "       'user_creation_to_tweet_time_days', 'user_tweeting_freq_per_day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {}\n",
    "for name, group in tweets.groupby('news_id'):\n",
    "    mappings[name] = group['tweet.user_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_columns = [\n",
    "       'tweet.user.protected',\n",
    "       'tweet.user.followers_count',\n",
    "       'tweet.user.friends_count',\n",
    "       'tweet.user.listed_count',\n",
    "       'tweet.user.favourites_count',\n",
    "       'tweet.user.verified',\n",
    "       'tweet.user.statuses_count',\n",
    "       'tweet.user.geo_enabled',\n",
    "       'tweet.user.default_profile',\n",
    "       'tweet.user.default_profile_image',\n",
    "       'username_words',\n",
    "       'description_words',\n",
    "       'user_creation_to_tweet_time_days',\n",
    "       'user_tweeting_freq_per_day'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reqd_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_tweet_info = {}\n",
    "for news in news_articles:\n",
    "    news_tweet_info[news] = []\n",
    "    try:\n",
    "        for user in mappings[news]:\n",
    "            current_user = tweets.loc[tweets['tweet.user_id'] == user]\n",
    "            data = []\n",
    "            if current_user.shape[0] != 0:\n",
    "                for col in reqd_columns:\n",
    "                    data.append(float(current_user[col].values[-1]))\n",
    "            else:\n",
    "                continue\n",
    "            news_tweet_info[news].append(data)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in news_tweet_info.keys():\n",
    "    with open(f'./tweet_user_features/{news}.pkl', 'wb') as f:\n",
    "        pkl.dump(news_tweet_info[news], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = []\n",
    "reqd_news = []\n",
    "for article in news_articles:\n",
    "    news = pkl.load(open(f'./tweet_user_features/{article}.pkl', 'rb'))\n",
    "    freq.append(len(news))\n",
    "    if len(news) > 2:\n",
    "        reqd_news.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I = 0 Length : 1639\n",
      "I = 1 Length : 1632\n",
      "I = 2 Length : 1627\n",
      "I = 3 Length : 1622\n",
      "I = 4 Length : 1618\n",
      "I = 5 Length : 1615\n",
      "I = 6 Length : 1610\n",
      "I = 7 Length : 1607\n",
      "I = 8 Length : 1604\n",
      "I = 9 Length : 1598\n",
      "I = 10 Length : 1591\n",
      "I = 11 Length : 1586\n",
      "I = 12 Length : 1579\n",
      "I = 13 Length : 1569\n",
      "I = 14 Length : 1562\n",
      "I = 15 Length : 1557\n",
      "I = 16 Length : 1549\n",
      "I = 17 Length : 1545\n",
      "I = 18 Length : 1539\n",
      "I = 19 Length : 1532\n",
      "I = 20 Length : 1525\n",
      "I = 21 Length : 1517\n",
      "I = 22 Length : 1507\n",
      "I = 23 Length : 1499\n",
      "I = 24 Length : 1493\n",
      "I = 25 Length : 1480\n",
      "I = 26 Length : 1474\n",
      "I = 27 Length : 1464\n",
      "I = 28 Length : 1459\n",
      "I = 29 Length : 1452\n",
      "I = 30 Length : 1445\n",
      "I = 31 Length : 1434\n",
      "I = 32 Length : 1426\n",
      "I = 33 Length : 1423\n",
      "I = 34 Length : 1417\n",
      "I = 35 Length : 1408\n",
      "I = 36 Length : 1399\n",
      "I = 37 Length : 1393\n",
      "I = 38 Length : 1390\n",
      "I = 39 Length : 1375\n",
      "I = 40 Length : 1370\n",
      "I = 41 Length : 1365\n",
      "I = 42 Length : 1358\n",
      "I = 43 Length : 1348\n",
      "I = 44 Length : 1340\n",
      "I = 45 Length : 1335\n",
      "I = 46 Length : 1332\n",
      "I = 47 Length : 1324\n",
      "I = 48 Length : 1319\n",
      "I = 49 Length : 1313\n",
      "I = 50 Length : 1308\n",
      "I = 51 Length : 1300\n",
      "I = 52 Length : 1295\n",
      "I = 53 Length : 1285\n",
      "I = 54 Length : 1276\n",
      "I = 55 Length : 1267\n",
      "I = 56 Length : 1256\n",
      "I = 57 Length : 1254\n",
      "I = 58 Length : 1249\n",
      "I = 59 Length : 1244\n",
      "I = 60 Length : 1235\n",
      "I = 61 Length : 1222\n",
      "I = 62 Length : 1212\n",
      "I = 63 Length : 1205\n",
      "I = 64 Length : 1198\n",
      "I = 65 Length : 1192\n",
      "I = 66 Length : 1187\n",
      "I = 67 Length : 1176\n",
      "I = 68 Length : 1173\n",
      "I = 69 Length : 1168\n",
      "I = 70 Length : 1160\n",
      "I = 71 Length : 1152\n",
      "I = 72 Length : 1147\n",
      "I = 73 Length : 1138\n",
      "I = 74 Length : 1128\n",
      "I = 75 Length : 1123\n",
      "I = 76 Length : 1119\n",
      "I = 77 Length : 1109\n",
      "I = 78 Length : 1095\n",
      "I = 79 Length : 1085\n",
      "I = 80 Length : 1077\n",
      "I = 81 Length : 1070\n",
      "I = 82 Length : 1065\n",
      "I = 83 Length : 1058\n",
      "I = 84 Length : 1053\n",
      "I = 85 Length : 1050\n",
      "I = 86 Length : 1044\n",
      "I = 87 Length : 1037\n",
      "I = 88 Length : 1028\n",
      "I = 89 Length : 1023\n",
      "I = 90 Length : 1018\n",
      "I = 91 Length : 1009\n",
      "I = 92 Length : 1002\n",
      "I = 93 Length : 995\n",
      "I = 94 Length : 981\n",
      "I = 95 Length : 974\n",
      "I = 96 Length : 968\n",
      "I = 97 Length : 961\n",
      "I = 98 Length : 954\n",
      "I = 99 Length : 946\n",
      "I = 100 Length : 939\n",
      "I = 101 Length : 931\n",
      "I = 102 Length : 923\n",
      "I = 103 Length : 912\n",
      "I = 104 Length : 905\n",
      "I = 105 Length : 902\n",
      "I = 106 Length : 893\n",
      "I = 107 Length : 888\n",
      "I = 108 Length : 881\n",
      "I = 109 Length : 878\n",
      "I = 110 Length : 874\n",
      "I = 111 Length : 870\n",
      "I = 112 Length : 861\n",
      "I = 113 Length : 854\n",
      "I = 114 Length : 843\n",
      "I = 115 Length : 836\n",
      "I = 116 Length : 830\n",
      "I = 117 Length : 824\n",
      "I = 118 Length : 820\n",
      "I = 119 Length : 813\n",
      "I = 120 Length : 809\n",
      "I = 121 Length : 803\n",
      "I = 122 Length : 799\n",
      "I = 123 Length : 793\n",
      "I = 124 Length : 789\n",
      "I = 125 Length : 781\n",
      "I = 126 Length : 773\n",
      "I = 127 Length : 767\n",
      "I = 128 Length : 760\n",
      "I = 129 Length : 755\n",
      "I = 130 Length : 753\n",
      "I = 131 Length : 750\n",
      "I = 132 Length : 747\n",
      "I = 133 Length : 739\n",
      "I = 134 Length : 735\n",
      "I = 135 Length : 731\n",
      "I = 136 Length : 729\n",
      "I = 137 Length : 724\n",
      "I = 138 Length : 722\n",
      "I = 139 Length : 715\n",
      "I = 140 Length : 713\n",
      "I = 141 Length : 707\n",
      "I = 142 Length : 703\n",
      "I = 143 Length : 699\n",
      "I = 144 Length : 694\n",
      "I = 145 Length : 691\n",
      "I = 146 Length : 684\n",
      "I = 147 Length : 683\n",
      "I = 148 Length : 680\n",
      "I = 149 Length : 675\n",
      "I = 150 Length : 673\n",
      "I = 151 Length : 671\n",
      "I = 152 Length : 664\n",
      "I = 153 Length : 660\n",
      "I = 154 Length : 658\n",
      "I = 155 Length : 655\n",
      "I = 156 Length : 651\n",
      "I = 157 Length : 649\n",
      "I = 158 Length : 645\n",
      "I = 159 Length : 641\n",
      "I = 160 Length : 638\n",
      "I = 161 Length : 637\n",
      "I = 162 Length : 632\n",
      "I = 163 Length : 629\n",
      "I = 164 Length : 623\n",
      "I = 165 Length : 622\n",
      "I = 166 Length : 619\n",
      "I = 167 Length : 617\n",
      "I = 168 Length : 611\n",
      "I = 169 Length : 604\n",
      "I = 170 Length : 599\n",
      "I = 171 Length : 598\n",
      "I = 172 Length : 597\n",
      "I = 173 Length : 591\n",
      "I = 174 Length : 583\n",
      "I = 175 Length : 581\n",
      "I = 176 Length : 577\n",
      "I = 177 Length : 576\n",
      "I = 178 Length : 569\n",
      "I = 179 Length : 566\n",
      "I = 180 Length : 563\n",
      "I = 181 Length : 555\n",
      "I = 182 Length : 551\n",
      "I = 183 Length : 547\n",
      "I = 184 Length : 540\n",
      "I = 185 Length : 540\n",
      "I = 186 Length : 535\n",
      "I = 187 Length : 532\n",
      "I = 188 Length : 529\n",
      "I = 189 Length : 521\n",
      "I = 190 Length : 519\n",
      "I = 191 Length : 517\n",
      "I = 192 Length : 516\n",
      "I = 193 Length : 515\n",
      "I = 194 Length : 511\n",
      "I = 195 Length : 507\n",
      "I = 196 Length : 503\n",
      "I = 197 Length : 502\n",
      "I = 198 Length : 498\n",
      "I = 199 Length : 493\n",
      "I = 200 Length : 487\n",
      "I = 201 Length : 485\n",
      "I = 202 Length : 481\n",
      "I = 203 Length : 480\n",
      "I = 204 Length : 476\n",
      "I = 205 Length : 472\n",
      "I = 206 Length : 471\n",
      "I = 207 Length : 468\n",
      "I = 208 Length : 466\n",
      "I = 209 Length : 462\n",
      "I = 210 Length : 461\n",
      "I = 211 Length : 460\n",
      "I = 212 Length : 457\n",
      "I = 213 Length : 452\n",
      "I = 214 Length : 448\n",
      "I = 215 Length : 443\n",
      "I = 216 Length : 438\n",
      "I = 217 Length : 438\n",
      "I = 218 Length : 436\n",
      "I = 219 Length : 433\n",
      "I = 220 Length : 433\n",
      "I = 221 Length : 429\n",
      "I = 222 Length : 427\n",
      "I = 223 Length : 426\n",
      "I = 224 Length : 426\n",
      "I = 225 Length : 426\n",
      "I = 226 Length : 423\n",
      "I = 227 Length : 422\n",
      "I = 228 Length : 420\n",
      "I = 229 Length : 416\n",
      "I = 230 Length : 415\n",
      "I = 231 Length : 415\n",
      "I = 232 Length : 415\n",
      "I = 233 Length : 412\n",
      "I = 234 Length : 410\n",
      "I = 235 Length : 407\n",
      "I = 236 Length : 405\n",
      "I = 237 Length : 404\n",
      "I = 238 Length : 402\n",
      "I = 239 Length : 401\n",
      "I = 240 Length : 400\n",
      "I = 241 Length : 400\n",
      "I = 242 Length : 400\n",
      "I = 243 Length : 398\n",
      "I = 244 Length : 394\n",
      "I = 245 Length : 393\n",
      "I = 246 Length : 391\n",
      "I = 247 Length : 391\n",
      "I = 248 Length : 390\n",
      "I = 249 Length : 386\n",
      "I = 250 Length : 383\n",
      "I = 251 Length : 380\n",
      "I = 252 Length : 380\n",
      "I = 253 Length : 380\n",
      "I = 254 Length : 379\n",
      "I = 255 Length : 378\n",
      "I = 256 Length : 376\n",
      "I = 257 Length : 375\n",
      "I = 258 Length : 373\n",
      "I = 259 Length : 367\n",
      "I = 260 Length : 367\n",
      "I = 261 Length : 366\n",
      "I = 262 Length : 365\n",
      "I = 263 Length : 364\n",
      "I = 264 Length : 363\n",
      "I = 265 Length : 363\n",
      "I = 266 Length : 360\n",
      "I = 267 Length : 358\n",
      "I = 268 Length : 355\n",
      "I = 269 Length : 355\n",
      "I = 270 Length : 352\n",
      "I = 271 Length : 349\n",
      "I = 272 Length : 348\n",
      "I = 273 Length : 344\n",
      "I = 274 Length : 344\n",
      "I = 275 Length : 344\n",
      "I = 276 Length : 343\n",
      "I = 277 Length : 341\n",
      "I = 278 Length : 340\n",
      "I = 279 Length : 336\n",
      "I = 280 Length : 334\n",
      "I = 281 Length : 334\n",
      "I = 282 Length : 329\n",
      "I = 283 Length : 329\n",
      "I = 284 Length : 327\n",
      "I = 285 Length : 326\n",
      "I = 286 Length : 324\n",
      "I = 287 Length : 323\n",
      "I = 288 Length : 322\n",
      "I = 289 Length : 318\n",
      "I = 290 Length : 316\n",
      "I = 291 Length : 314\n",
      "I = 292 Length : 313\n",
      "I = 293 Length : 313\n",
      "I = 294 Length : 310\n",
      "I = 295 Length : 308\n",
      "I = 296 Length : 305\n",
      "I = 297 Length : 304\n",
      "I = 298 Length : 304\n",
      "I = 299 Length : 303\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    print(f'I = {i} Length : {len([i for j in freq if j >= i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.array(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211.63453325198293"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_tweet_data = []\n",
    "tweet_user_size = 64   #can change this\n",
    "for news in reqd_news:\n",
    "    data = pkl.load(open(f'./tweet_user_features/{news}.pkl', 'rb'))\n",
    "    if len(data) >= tweet_user_size:\n",
    "        graph_tweet_data.append(data[:tweet_user_size])\n",
    "    else:\n",
    "        data = np.asarray(data)\n",
    "        random_ = data[np.random.choice(data.shape[0], tweet_user_size, replace=True),:]\n",
    "        random_ = random_.tolist()\n",
    "        data = data.tolist()\n",
    "        data.extend(random_)\n",
    "        graph_tweet_data.append(data[:tweet_user_size])\n",
    "save('graph_tweet_data.npy', graph_tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_user_adj_mat = []  # Cosine, euclidean, manhattan, average distance, weighted euclidean\n",
    "for user_adj_mat in graph_tweet_data:   \n",
    "    user_adj_mat =  np.array(user_adj_mat)\n",
    "    user_adj_mat = sparse.csr_matrix(user_adj_mat)\n",
    "    user_adj_mat = cosine_similarity(user_adj_mat)\n",
    "    cosine_sim_user_adj_mat.append(user_adj_mat)                    # Contains the adjaceny matrix for each news article\n",
    "save('cosine_similarity_graph_tweet.npy', cosine_sim_user_adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleanup(data):\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'^RT[\\s]+', '', data)\n",
    "    data = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', data)\n",
    "    data = re.sub(r'#', '', data)\n",
    "    data = re.sub(r'[0-9]', '', data)\n",
    "    data = re.sub(r'@[a-zA-Z0-9_]*', '', data)\n",
    "    data = re.sub(r':', '', data)\n",
    "    data = re.sub(r'[,.\\'“-]', '', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_texts_required = [] \n",
    "for news in reqd_news:\n",
    "    tweet_ = tweets.loc[tweets['news_id'] == news]['tweet.text'].values[0]\n",
    "    tweet_texts_required.append(basic_cleanup(tweet_))\n",
    "\n",
    "tweet_texts_required_file = open('tweet_texts_required.pkl', 'wb')\n",
    "pkl.dump(tweet_texts_required, tweet_texts_required_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median lenth of text (retweet):  12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 37., 220., 607., 326., 230., 189.,  11.,   1.,   0.,   1.]),\n",
       " array([ 1. ,  4.8,  8.6, 12.4, 16.2, 20. , 23.8, 27.6, 31.4, 35.2, 39. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open(\"./tweet_texts_required.pkl\", \"rb\")\n",
    "tweet_texts = pkl.load(f)\n",
    "sequence_len = np.asarray(list(map(lambda x: len(x.split(' ')), tweet_texts)))\n",
    "print(\"Median lenth of text (retweet): \", np.median(sequence_len))\n",
    "plt.hist(sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokens = tokenizer.fit_on_texts(tweet_texts)\n",
    "sequences = tokenizer.texts_to_sequences(tweet_texts)\n",
    "padded = pad_sequences(sequences, maxlen=14, padding='post')  # maxlen can be changed based on the plot above\n",
    "save('tweet_padded_token.npy', padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('../data/HealthStory_reviews.csv')\n",
    "reviews['verdict'] = reviews['rating'].apply(lambda x: int(x >= 3))\n",
    "labels = []\n",
    "for news in reqd_news:\n",
    "    current_review = reviews[reviews['news_id'] == news]['verdict'].values[0]\n",
    "    labels.append(current_review)\n",
    "save('news_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj_numpy(adj, symmetric=True):\n",
    "    if symmetric:\n",
    "        d = np.diag(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
    "        a_norm = adj.dot(d).transpose().dot(d)\n",
    "    else:\n",
    "        d = np.diag(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
    "        a_norm = d.dot(adj)\n",
    "    return a_norm\n",
    "\n",
    "def preprocess_adj_tensor(adj_tensor, symmetric=True):\n",
    "    adj_out_tensor = []\n",
    "    for i in range(adj_tensor.shape[0]):\n",
    "        adj = adj_tensor[i]\n",
    "        adj = adj + np.eye(adj.shape[0])\n",
    "        adj = normalize_adj_numpy(adj, symmetric)        \n",
    "        adj_out_tensor.append(adj)\n",
    "    adj_out_tensor = np.array(adj_out_tensor)\n",
    "    return adj_out_tensor\n",
    "\n",
    "def graph_conv_op(x, num_filters, graph_conv_filters, kernel):\n",
    "\n",
    "    if len(x.get_shape()) == 2:\n",
    "        conv_op = K.dot(graph_conv_filters, x)\n",
    "        conv_op = tf.split(conv_op, num_filters, axis=0)\n",
    "        conv_op = K.concatenate(conv_op, axis=1)\n",
    "    elif len(x.get_shape()) == 3:\n",
    "        conv_op = K.batch_dot(graph_conv_filters, x)\n",
    "        conv_op = tf.split(conv_op, num_filters, axis=1)\n",
    "        conv_op = K.concatenate(conv_op, axis=2)\n",
    "    else:\n",
    "        raise ValueError('x must be either 2 or 3 dimension tensor'\n",
    "                         'Got input shape: ' + str(x.get_shape()))\n",
    "\n",
    "    conv_out = K.dot(conv_op, kernel)\n",
    "    return conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3b3b95debdb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 1                  # we should not be able to change this\n",
    "source_tweet_output_dim=64       #  \n",
    "source_tweet_length=14           # obtained from the plot in cell 12\n",
    "number_of_feature=14             # same as reqd columns\n",
    "filter_size=3                     \n",
    "output_dim=64                  \n",
    "vocab_size = 4050                # Used in word embeddings module\n",
    "tweet_user_size = 64             # obtained from the 1st plot\n",
    "GCN_output_dim = 8               # Initializing Mulitgraphcnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGraphCNN(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 num_filters,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',  # can be changed\n",
    "                 bias_initializer='zeros',             # can be changed\n",
    "                 kernel_regularizer=None,              # can be changed\n",
    "                 bias_regularizer=None,                # can be changed\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(MultiGraphCNN, self).__init__(**kwargs)\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_initializer.__name__ = kernel_initializer\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        if self.num_filters != int(input_shape[1][-2]/input_shape[1][-1]):\n",
    "            raise ValueError('num_filters does not match with graph_conv_filters dimensions.')\n",
    "\n",
    "        self.input_dim = input_shape[0][-1]\n",
    "        kernel_shape = (self.num_filters * self.input_dim, self.output_dim)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.output_dim,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        output = graph_conv_op(inputs[0], self.num_filters, inputs[1], self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "        return output_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'num_filters': self.num_filters,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)  \n",
    "        }\n",
    "        base_config = super(MultiGraphCNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = load('cosine_similarity_graph_tweet.npy')\n",
    "graph_conv_filters = preprocess_adj_tensor(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load('news_labels.npy')\n",
    "padded_docs = load('tweet_padded_token.npy')\n",
    "graph_retweet_data = load('graph_tweet_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(graph_retweet_data, y, test_size=0.3, random_state=54)\n",
    "WX_train, WX_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.3, random_state=54 )\n",
    "MX_train, MX_test, y_train, y_test = train_test_split(graph_conv_filters, y, test_size=0.3, random_state=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tweet_output_dim = 128 # can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "winput = tf.keras.layers.Input(shape=(source_tweet_length,))\n",
    "wembed = tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    source_tweet_output_dim,\n",
    "    input_length = source_tweet_length,\n",
    "    embeddings_initializer='glorot_uniform', # can be changed \n",
    "    embeddings_regularizer='l1_l2',          # can be changed   \n",
    "     )(winput)\n",
    "wembed = tf.keras.layers.GRU(source_tweet_output_dim, return_sequences=True)(wembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmain stores the user feature matrix\n",
    "rmain_input = tf.keras.layers.Input(shape=(tweet_user_size, number_of_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conv_filters_input = tf.keras.layers.Input(shape=(tweet_user_size, tweet_user_size))\n",
    "gmain_input= MultiGraphCNN(GCN_output_dim, num_filters)([rmain_input, graph_conv_filters_input])\n",
    "#gmain_input= MultiGraphCNN(GCN_output_dim, num_filters)([gmain_input, graph_conv_filters_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = MultiHeadAttention(num_heads=10, key_dim=2, value_dim=2, dropout=0.3, attention_axes=(1,2))\n",
    "output_tensor, weights = layer(wembed, gmain_input, \n",
    "                               return_attention_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 14)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 14, 128)      518400      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 64, 14)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 64, 64)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 14, 128)      99072       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multi_graph_cnn (MultiGraphCNN) (None, 64, 8)        120         input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead ((None, 14, 128), (N 5628        gru[0][0]                        \n",
      "                                                                 multi_graph_cnn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 14, 64)       8256        multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 14, 64)       4160        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 14, 1)        65          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 635,701\n",
      "Trainable params: 635,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#dropout layers can be added\n",
    "x=tf.keras.layers.Dense(output_dim,activation=\"relu\")(output_tensor)\n",
    "x=tf.keras.layers.Dense(output_dim,activation=\"relu\")(x)\n",
    "prediction= tf.keras.layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model([winput, rmain_input, graph_conv_filters_input],prediction)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam= tf.keras.optimizers.Adam(lr=0.001, beta_1=0.7, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5, verbose=2)\n",
    "model.compile(optimizer=Adam ,loss=\"binary_crossentropy\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": " [_Derived_]RecvAsync is cancelled.\n\t [[{{node Adam/Adam/update/ResourceApplyAdam/_77}}]]\n\t [[Adam/gradients/AddN_4/_74]] [Op:__inference_train_function_4328]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-62b327c16f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m:  [_Derived_]RecvAsync is cancelled.\n\t [[{{node Adam/Adam/update/ResourceApplyAdam/_77}}]]\n\t [[Adam/gradients/AddN_4/_74]] [Op:__inference_train_function_4328]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([np.array(WX_train),np.array(X_train),np.array(MX_train)],np.array(y_train),epochs=40,validation_split=0.3, callbacks =[early_stopping], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=model.evaluate([np.array(WX_test),np.array(X_test),np.array(MX_test)],np.array(y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fakenews]",
   "language": "python",
   "name": "conda-env-fakenews-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
